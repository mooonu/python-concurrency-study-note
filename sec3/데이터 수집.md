## 동시성 프로그래밍으로 크롤링

### Code
- 각 페이지의 데이터를 순차적으로 가져오는 것이 아닌 동시에 가져오는 Fetch 함수 만들기

```python
from bs4 import BeautifulSoup
import aiohttp
import asyncio


async def fetch(session, url, i):
    print(i + 1)
    async with session.get(url) as response:
        html = await response.text()
        soup = BeautifulSoup(html, "html.parser")
        cont_thumb = soup.find_all("div", "cont_thumb")
        for cont in cont_thumb:
            title = cont.find("p", "txt_thumb")
            if title is not None:
                print(title.text)


async def main():
    BASE_URL = "https://bjpublic.tistory.com/category/%EC%A0%84%EC%B2%B4%20%EC%B6%9C%EA%B0%84%20%EB%8F%84%EC%84%9C"
    urls = [f"{BASE_URL}?page={i}" for i in range(1, 10)]
    async with aiohttp.ClientSession() as session:
        await asyncio.gather(*[fetch(session, url, i) for i, url in enumerate(urls)])


if __name__ == "__main__":
    asyncio.run(main())

```

### Lesson
- API가 없을 때는 예전처럼 이렇게 순수 노가다 및 가공
- 동시에 데이터를 요청하고 가져온 후 리턴

---

### Code
- OpenAPI 활용 이미지 링크 수집한 뒤, 로컬 컴퓨터에 동시성 프로그래밍 적용해 다운로드

```python
import os
import aiohttp
import asyncio
from config import get_secret
import aiofiles


async def img_downloader(session, img):
    img_name = img.split("/")[-1].split("?")[0]
    
    # 파일명 최대 길이 제한
    if len(img_name) > 50:
        ext = os.path.splitext(img_name)[1]
        img_name = img_name[:50 - len(ext)] + ext

    os.makedirs("./images", exist_ok=True)

    async with session.get(img) as response:
        if response.status == 200:
            async with aiofiles.open(f"./images/{img_name}", mode="wb") as file:
                img_data = await response.read()
                await file.write(img_data)


async def fetch(session, url, i):
    print(i + 1)
    headers = {
        "X-Naver-Client-Id": get_secret("NAVER_API_ID"),
        "X-Naver-Client-Secret": get_secret("NAVER_API_SECRET"),
    }

    async with session.get(url, headers=headers) as response:
        result = await response.json()
        items = result["items"]
        images = [item["link"] for item in items]
        await asyncio.gather(*[img_downloader(session, img) for img in images])


async def main():
    BASE_URL = "https://openapi.naver.com/v1/search/image"
    keyword = "cat"
    urls = [f"{BASE_URL}?query={keyword}&display=20&start={1 + i * 20}" for i in range(2)]

    async with aiohttp.ClientSession() as session:
        await asyncio.gather(*[fetch(session, url, i) for i, url in enumerate(urls)])


if __name__ == "__main__":
    asyncio.run(main())

```

### Lesson
- API id,secret의 경우 secret.json에 저장
- config.py로 secret.json 해석

---

## 활용

### Code
- input으로 검색어를 받아 output으로 로컬에 데이터를 다운로드하기

```python
async def main():
    BASE_URL = "https://openapi.naver.com/v1/search/image"
    keyword = input()
```